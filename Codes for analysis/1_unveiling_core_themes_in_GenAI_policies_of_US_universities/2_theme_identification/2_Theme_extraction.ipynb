{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b21735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from theme_identifier import *\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "file_path=\"../results/interim_results/summaries.json\"\n",
    "summaries=load_dict(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76eb7868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db99373778034f74ad8ff3913dbd1e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAIN STARTING\n",
      "\n",
      "--------------------------------------------------\n",
      "CHECKING FOR QUALITY\n",
      "--------------------------------------------------\n",
      "QUALITY CHECK PASSED!\n",
      "--------------------------------------------------\n",
      "CHECKING FOR HALLUCINATION\n",
      "--------------------------------------------------\n",
      "HALLUCINATION CHECK PASSED!\n",
      "CHAIN STARTING\n",
      "\n",
      "--------------------------------------------------\n",
      "CHECKING FOR QUALITY\n",
      "--------------------------------------------------\n",
      "QUALITY CHECK PASSED!\n",
      "--------------------------------------------------\n",
      "CHECKING FOR HALLUCINATION\n",
      "--------------------------------------------------\n",
      "HALLUCINATION CHECK PASSED!\n",
      "CHAIN STARTING\n",
      "\n",
      "--------------------------------------------------\n",
      "CHECKING FOR QUALITY\n",
      "--------------------------------------------------\n",
      "QUALITY CHECK PASSED!\n",
      "--------------------------------------------------\n",
      "CHECKING FOR HALLUCINATION\n",
      "--------------------------------------------------\n",
      "HALLUCINATION CHECK PASSED!\n",
      "CHAIN STARTING\n",
      "\n",
      "--------------------------------------------------\n",
      "CHECKING FOR QUALITY\n",
      "--------------------------------------------------\n",
      "QUALITY CHECK PASSED!\n",
      "--------------------------------------------------\n",
      "CHECKING FOR HALLUCINATION\n",
      "--------------------------------------------------\n",
      "HALLUCINATION CHECK PASSED!\n",
      "CHAIN STARTING\n",
      "\n",
      "--------------------------------------------------\n",
      "CHECKING FOR QUALITY\n",
      "--------------------------------------------------\n",
      "QUALITY CHECK PASSED!\n",
      "--------------------------------------------------\n",
      "CHECKING FOR HALLUCINATION\n",
      "--------------------------------------------------\n",
      "HALLUCINATION CHECK PASSED!\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(5)):\n",
    "    curr_dict = jumble_dict_order(summaries)  # Jumble the dictionary\n",
    "    identified_points = chain_prompts(str(curr_dict))\n",
    "    data_to_save = identified_points + '\\n---\\n'\n",
    "    # Save the data\n",
    "    save_to_file(data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a4d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Key Point**: Academic integrity and responsible AI use  \n",
      "   **Explanation**: Most universities emphasize the importance of maintaining academic integrity in the context of AI use. This includes clear guidelines on what constitutes acceptable and unacceptable use of AI tools, with a focus on preventing plagiarism and unauthorized assistance. Institutions encourage transparency in disclosing AI usage in academic work and stress the need for original contributions from students.  \n",
      "   **Examples**: The University of Notre Dame and New York University both highlight that misrepresentation of AI-generated work as original constitutes academic dishonesty. Similarly, Carnegie Mellon University and Duke University require students to clarify AI usage in their assignments to uphold academic integrity.\n",
      "\n",
      "2. **Key Point**: Instructor discretion and course-specific policies  \n",
      "   **Explanation**: Many universities grant instructors the authority to define their own policies regarding AI use in their courses. This allows for flexibility in how AI tools are integrated into the curriculum based on specific learning objectives and course requirements. Instructors are encouraged to communicate their policies clearly in syllabi to avoid confusion among students.  \n",
      "   **Examples**: At Stanford University and Princeton University, faculty members are responsible for establishing their own guidelines on AI use, while Georgetown University encourages instructors to clarify acceptable AI use in their courses.\n",
      "\n",
      "3. **Key Point**: Transparency and documentation of AI usage  \n",
      "   **Explanation**: A common theme across institutions is the requirement for students to disclose their use of AI tools in academic work. This includes documenting the extent of AI assistance and providing proper citations for AI-generated content. Transparency is seen as essential for maintaining trust and integrity in academic submissions.  \n",
      "   **Examples**: California Institute of Technology and the University of California, San Diego both require users to disclose their use of generative AI tools to avoid misunderstandings. Similarly, the University of Michigan emphasizes the need for students to document AI tool usage in their assignments.\n",
      "\n",
      "4. **Key Point**: Data privacy and security considerations  \n",
      "   **Explanation**: Universities are increasingly aware of the privacy and security risks associated with using AI tools, particularly regarding sensitive or confidential information. Policies often prohibit entering personal data into AI systems without proper safeguards and emphasize compliance with data protection regulations.  \n",
      "   **Examples**: The University of Wisconsin–Madison and Harvard University both highlight the importance of not entering confidential data into publicly available AI tools. Additionally, the University of California, Santa Barbara stresses the need for compliance with security practices when using AI tools.\n",
      "\n",
      "5. **Key Point**: Ethical considerations and critical engagement with AI  \n",
      "   **Explanation**: Institutions encourage students and faculty to critically evaluate AI-generated content for accuracy, bias, and ethical implications. This includes discussions about the limitations of AI tools and the potential for biases in outputs. Universities aim to foster a culture of inquiry and ethical responsibility in the use of AI technologies.  \n",
      "   **Examples**: Emory University and the University of Virginia both emphasize the importance of ethical considerations in AI use, addressing issues such as bias and privacy. Similarly, the University of California, Davis encourages critical engagement with AI outputs to ensure responsible usage.\n",
      "\n",
      "6. **Key Point**: Continuous evaluation and adaptation of AI policies  \n",
      "   **Explanation**: Many universities recognize the rapidly evolving nature of AI technologies and commit to regularly reviewing and updating their policies to reflect new developments and best practices. This ensures that guidelines remain relevant and effective in addressing the challenges posed by AI in academic settings.  \n",
      "   **Examples**: Tufts University and the University of California, Berkeley both state that their AI policies will evolve alongside technological advancements. Additionally, the University of Michigan emphasizes the need for ongoing adaptation of AI policies as technology develops.\n",
      "---\n",
      "1. **Key Point**: **Academic integrity and responsible AI use**  \n",
      "   **Explanation**: Many universities emphasize the importance of maintaining academic integrity in the context of AI tool usage. Institutions are concerned about unauthorized assistance, plagiarism, and the need for clear guidelines on acceptable AI use in academic work. This theme reflects a collective effort to ensure that students understand the implications of using AI tools and the necessity of original work.  \n",
      "   **Examples**: Boston University and Northwestern University require instructors to communicate their expectations regarding AI use in assignments. Similarly, Duke University and Stanford University highlight that unauthorized use of AI tools is considered cheating under their academic integrity policies.\n",
      "\n",
      "2. **Key Point**: **Clear communication of AI policies**  \n",
      "   **Explanation**: Institutions stress the need for clear communication regarding AI usage policies in course syllabi. Faculty are encouraged to articulate their stance on AI tools, including permissible and prohibited uses, to prevent misunderstandings among students. This theme underscores the importance of transparency in academic settings.  \n",
      "   **Examples**: Harvard University and the University of Pennsylvania encourage instructors to include specific AI usage guidelines in their syllabi. The University of California, Santa Barbara also emphasizes the need for instructors to communicate expectations clearly regarding AI tool usage.\n",
      "\n",
      "3. **Key Point**: **Data privacy and security**  \n",
      "   **Explanation**: A significant concern across universities is the protection of sensitive data when using AI tools. Institutions highlight the importance of not entering confidential or personal information into generative AI systems and ensuring compliance with data protection regulations. This theme reflects a commitment to safeguarding user data and maintaining privacy standards.  \n",
      "   **Examples**: Northwestern University and the University of Florida prohibit the input of sensitive data into AI tools. Similarly, the University of California, Davis and Emory University emphasize the need for users to be cautious about data privacy when utilizing AI technologies.\n",
      "\n",
      "4. **Key Point**: **Ethical considerations in AI use**  \n",
      "   **Explanation**: Many universities address the ethical implications of using AI tools, including issues related to bias, misinformation, and the potential for AI to perpetuate existing inequalities. Institutions encourage critical engagement with AI outputs and discussions about the ethical dimensions of AI in academic contexts.  \n",
      "   **Examples**: Yale University and the University of Virginia promote discussions about ethical considerations in AI use, urging both faculty and students to reflect on biases and inaccuracies in AI-generated content. Emory University also emphasizes the importance of ethical data usage and the implications of AI technologies.\n",
      "\n",
      "5. **Key Point**: **Instructor discretion and flexibility**  \n",
      "   **Explanation**: Institutions recognize the need for instructors to have the flexibility to define their own policies regarding AI use in their courses. This theme highlights the importance of faculty autonomy in determining how AI tools can be integrated into their teaching while aligning with course objectives.  \n",
      "   **Examples**: At the University of California, Los Angeles and Princeton University, faculty members have the authority to set specific guidelines for AI use in their courses. Similarly, Dartmouth College allows instructors to permit or prohibit AI tools based on pedagogical value.\n",
      "\n",
      "6. **Key Point**: **Ongoing evaluation and adaptation of policies**  \n",
      "   **Explanation**: Many universities acknowledge that AI technology is rapidly evolving and that policies must be regularly reviewed and updated to remain relevant. This theme reflects a proactive approach to adapting to new developments in AI and ensuring that institutional guidelines align with best practices.  \n",
      "   **Examples**: The University of Michigan and the University of Wisconsin–Madison emphasize the need for continuous evaluation of AI policies as technology advances. Harvard University also commits to monitoring developments in generative AI and updating guidelines accordingly.\n",
      "\n",
      "7. **Key Point**: **Resources and support for AI integration**  \n",
      "   **Explanation**: Institutions are providing resources and support to help faculty and students navigate the complexities of AI tool usage. This theme highlights the importance of training, workshops, and consultations to foster responsible AI use in academic settings.  \n",
      "   **Examples**: Boston University and the University of Florida offer workshops and resources for faculty on integrating AI into teaching. Similarly, the University of California, San Diego provides training and guidelines for responsible AI use.\n",
      "---\n",
      "Here is the refined grouping of the key themes associated with AI policies in higher education, preserving the original ranking as requested:\n",
      "\n",
      "1. **Key Point**: Academic integrity and responsible AI use  \n",
      "   **Explanation**: Many universities emphasize the importance of maintaining academic integrity when using AI tools. This includes clear guidelines on what constitutes acceptable and unacceptable use of AI in academic work, with a focus on preventing plagiarism and unauthorized assistance. Institutions encourage students to disclose AI usage and ensure that their work remains original.  \n",
      "   **Examples**: Georgetown University and Harvard University both highlight that using AI-generated content without proper citation is considered plagiarism. Similarly, the University of Pennsylvania and Duke University stress the need for students to disclose AI tool usage in their assignments.\n",
      "\n",
      "2. **Key Point**: Clear communication of AI policies  \n",
      "   **Explanation**: Institutions consistently advocate for clear communication regarding AI usage policies in course syllabi. Faculty are encouraged to articulate their expectations for AI use, ensuring that students understand the guidelines and implications of using AI tools in their coursework.  \n",
      "   **Examples**: Universities like Stanford and the University of Michigan encourage instructors to include specific AI usage guidelines in their syllabi. The University of California, Davis also emphasizes the need for instructors to establish clear policies regarding AI use in coursework.\n",
      "\n",
      "3. **Key Point**: Data privacy and security concerns  \n",
      "   **Explanation**: A significant number of universities address the importance of data privacy and security when using AI tools. Policies often prohibit the input of sensitive or confidential information into AI systems, emphasizing the need for compliance with privacy regulations.  \n",
      "   **Examples**: Harvard University and the University of California, Santa Barbara both stress that users must avoid entering personal or sensitive data into AI tools. Similarly, the University of Florida and Northwestern University highlight the necessity of protecting institutional data when using generative AI.\n",
      "\n",
      "4. **Key Point**: Ethical considerations in AI use  \n",
      "   **Explanation**: Many institutions recognize the ethical implications of using AI tools, including issues related to bias, misinformation, and the potential for AI to perpetuate existing inequalities. Universities encourage critical engagement with AI outputs and discussions about the ethical use of these technologies.  \n",
      "   **Examples**: Emory University and the University of Virginia both emphasize the need for ethical considerations in AI applications, addressing topics such as bias and privacy. The University of California, Los Angeles also encourages ongoing evaluation of AI systems for fairness and transparency.\n",
      "\n",
      "5. **Key Point**: Continuous adaptation and policy evolution  \n",
      "   **Explanation**: Institutions acknowledge the rapidly evolving nature of AI technology and the need for ongoing updates to policies and practices. Many universities commit to regularly reviewing and adapting their AI guidelines to reflect advancements in technology and educational needs.  \n",
      "   **Examples**: The University of Washington and the University of Notre Dame both mention the importance of continuously updating AI policies as technology evolves. Similarly, Texas A&M University and the University of Michigan emphasize the need for ongoing evaluation of AI tools and practices.\n",
      "\n",
      "6. **Key Point**: Documentation and transparency in AI usage  \n",
      "   **Explanation**: Several universities require students and faculty to document their use of AI tools, ensuring transparency in how AI contributes to academic work. This includes providing prompts, outputs, and any modifications made to AI-generated content.  \n",
      "   **Examples**: The University of Illinois Urbana-Champaign mandates that students document their use of generative AI, including prompts and outputs. Similarly, Carnegie Mellon University and the University of California, San Diego emphasize the importance of transparency in AI tool usage.\n",
      "\n",
      "7. **Key Point**: Instructor discretion in AI integration  \n",
      "   **Explanation**: Many institutions grant instructors the authority to define their own policies regarding AI use in their courses. This allows for flexibility in how AI tools are integrated into teaching, depending on the specific context and learning objectives.  \n",
      "   **Examples**: Duke University and Princeton University both allow instructors to set their own guidelines for AI use, while the University of Southern California notes that individual professors can establish their own policies regarding AI in academic work.\n",
      "\n",
      "This refined list maintains the original ranking and presents a comprehensive overview of the themes related to AI policies in higher education.\n",
      "---\n",
      "1. **Key Point**: **Academic integrity and responsible AI use**  \n",
      "   **Explanation**: Many universities emphasize the importance of academic integrity in the context of AI usage. Policies often highlight that unauthorized use of AI tools is considered academic dishonesty, and students must ensure that their work is original and properly attributed. Institutions encourage clear communication of expectations regarding AI use in course syllabi to prevent misunderstandings.  \n",
      "   **Examples**: Universities such as Princeton, Yale, and Johns Hopkins require students to disclose AI usage in their assignments and clarify the implications of unauthorized use. Additionally, many institutions, including Stanford and the University of Florida, stress the need for students to understand and adhere to academic integrity policies related to AI.\n",
      "\n",
      "2. **Key Point**: **Data privacy and security**  \n",
      "   **Explanation**: A significant number of universities have established guidelines to protect sensitive and confidential data when using AI tools. Policies commonly prohibit the input of personally identifiable information (PII) or sensitive institutional data into generative AI systems, emphasizing the need for compliance with privacy regulations.  \n",
      "   **Examples**: Institutions like Washington University in St. Louis and the University of Michigan explicitly state that sensitive data must not be entered into AI tools. Similarly, Georgia Tech and the University of California, Santa Barbara highlight the importance of data protection and compliance with security practices.\n",
      "\n",
      "3. **Key Point**: **Clear communication of AI policies**  \n",
      "   **Explanation**: Universities frequently stress the necessity for instructors to clearly communicate their policies regarding AI use in course syllabi. This includes outlining acceptable and prohibited uses of AI tools, which helps to set expectations for students and fosters a transparent learning environment.  \n",
      "   **Examples**: Institutions such as Duke University and the University of Pennsylvania encourage faculty to provide specific guidelines on AI usage in their courses. Similarly, the University of California, Los Angeles and Boston University advocate for clear syllabus statements to guide students on acceptable AI practices.\n",
      "\n",
      "4. **Key Point**: **Ethical considerations and responsible AI use**  \n",
      "   **Explanation**: Many universities recognize the ethical implications of using AI tools, including issues related to bias, misinformation, and the potential for AI to perpetuate existing inequalities. Institutions encourage discussions about these ethical concerns and promote responsible use of AI technologies.  \n",
      "   **Examples**: Emory University and the University of Virginia emphasize the importance of ethical considerations in AI applications, while Northwestern University and the University of California, Davis encourage critical evaluation of AI-generated content for biases and inaccuracies.\n",
      "\n",
      "5. **Key Point**: **Training and resources for AI integration**  \n",
      "   **Explanation**: A number of universities provide resources, workshops, and training opportunities to help faculty and students effectively integrate AI tools into their teaching and research practices. This support aims to enhance understanding of AI technologies and promote responsible usage.  \n",
      "   **Examples**: Institutions like the University of Michigan and Texas A&M University offer extensive training on generative AI for staff and students. Similarly, the University of Notre Dame and Harvard University provide resources and workshops to assist faculty in navigating AI integration in their courses.\n",
      "\n",
      "6. **Key Point**: **Ongoing evaluation and adaptation of policies**  \n",
      "   **Explanation**: Many universities acknowledge the rapidly evolving nature of AI technologies and commit to regularly reviewing and updating their policies to reflect advancements in the field. This ensures that guidelines remain relevant and effective in addressing new challenges posed by AI.  \n",
      "   **Examples**: Institutions such as Yale University and the University of California, San Diego emphasize the need for continuous evaluation of AI policies. Similarly, the University of Wisconsin–Madison and Carnegie Mellon University highlight the importance of adapting guidelines as technology evolves.\n",
      "---\n",
      "1. **Key Point**: **Academic integrity and responsible AI use**  \n",
      "   **Explanation**: Many universities emphasize the importance of maintaining academic integrity in the context of AI tools. This includes clear guidelines on what constitutes acceptable use of AI, the necessity for students to disclose AI assistance, and the consequences of unauthorized use. Institutions are focused on fostering a culture of honesty and transparency regarding AI-generated content.  \n",
      "   **Examples**: Princeton University requires students to disclose AI use in assignments, while Stanford University emphasizes that using AI for substantial completion of assignments is prohibited unless explicitly stated by instructors. Similarly, the University of Virginia highlights the need for strategies to prevent academic dishonesty related to AI tools.\n",
      "\n",
      "2. **Key Point**: **Clear guidelines for AI tool usage**  \n",
      "   **Explanation**: Institutions are increasingly developing specific policies and guidelines regarding the use of AI tools in academic settings. These guidelines often include expectations for both faculty and students, detailing permissible and prohibited uses of AI in coursework. This clarity aims to prevent misunderstandings and ensure that all stakeholders are aware of their responsibilities.  \n",
      "   **Examples**: Georgetown University encourages faculty to establish clear policies on AI usage in course syllabi, while Vanderbilt University mandates that faculty communicate their expectations regarding AI use to students.\n",
      "\n",
      "3. **Key Point**: **Instructor discretion and course-specific policies**  \n",
      "   **Explanation**: Many universities grant instructors the authority to define the use of AI tools in their courses. This allows for flexibility in addressing the unique needs of different disciplines and teaching styles. Instructors are encouraged to communicate their specific policies clearly to students.  \n",
      "   **Examples**: Duke University allows instructors to set their own guidelines for AI use, while the University of Michigan encourages faculty to communicate their AI policies in syllabi.\n",
      "\n",
      "4. **Key Point**: **Documentation and transparency in AI use**  \n",
      "   **Explanation**: Universities are emphasizing the need for transparency regarding AI tool usage. Students are often required to document their use of AI in assignments, including proper citations and acknowledgment of AI contributions. This practice aims to uphold academic integrity and accountability.  \n",
      "   **Examples**: The University of Illinois Urbana-Champaign mandates that students document their use of AI tools in their work, while Yale University requires students to cite any AI-generated content used in their coursework.\n",
      "\n",
      "5. **Key Point**: **Ethical considerations and biases in AI**  \n",
      "   **Explanation**: Many universities recognize the ethical implications of using AI tools, particularly concerning biases inherent in AI outputs and the potential for misinformation. Institutions are promoting critical engagement with AI-generated content and encouraging users to evaluate the accuracy and reliability of AI outputs.  \n",
      "   **Examples**: The University of Chicago advises users to verify the accuracy of AI-generated content, while Johns Hopkins University emphasizes the importance of being aware of potential biases in AI outputs.\n",
      "\n",
      "6. **Key Point**: **Data privacy and security**  \n",
      "   **Explanation**: A significant concern across universities is the protection of sensitive data when using AI tools. Institutions are implementing guidelines to ensure that confidential or personal information is not entered into AI systems, and they are emphasizing compliance with data protection regulations.  \n",
      "   **Examples**: Harvard University prohibits entering confidential data into publicly available AI tools, while the University of California, Santa Barbara advises users to avoid inputting sensitive information into ChatGPT.\n",
      "\n",
      "7. **Key Point**: **Training and resources for AI integration**  \n",
      "   **Explanation**: Institutions are providing resources and training to help faculty and students effectively integrate AI tools into their academic practices. This includes workshops, consultations, and guidelines aimed at enhancing understanding and responsible use of AI technologies.  \n",
      "   **Examples**: Georgetown University offers resources and consultations for faculty on integrating AI into teaching, while the University of Maryland provides workshops and support for faculty to adapt instructional approaches to leverage AI.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "read_and_print_file()  # This will read and print the content of 'key_points.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00153ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
