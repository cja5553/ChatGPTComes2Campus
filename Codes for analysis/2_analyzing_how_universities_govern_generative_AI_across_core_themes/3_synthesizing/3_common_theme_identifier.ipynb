{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64359ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthesizer import *\n",
    "import json\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "with open(\"../2_identifying/analyses.json\", \"r\") as file:\n",
    "    query_answers = json.load(file)\n",
    "\n",
    "for i in (list(query_answers.keys())):\n",
    "    query_answers[i]=list(query_answers[i])\n",
    "\n",
    "queries=list(query_answers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0f60f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40f6eed8bba436d8b518958cc653fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination check: PASS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hallucination check: PASS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hallucination check: PASS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hallucination check: PASS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hallucination check: PASS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hallucination check: PASS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hallucination check: PASS\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "synthesized_policies={}\n",
    "for query in tqdm_notebook(queries):\n",
    "    policies=query_answers[query]\n",
    "    # # Step 2: Get common themes from the model\n",
    "    common_themes = synthesize_common_policies(policies, query)\n",
    "    # # Step 3: Clean the output (optional)\n",
    "    synthesized_policies[query]=common_themes\n",
    "    with open('synthesized_policies.json', 'w') as f:\n",
    "        json.dump(synthesized_policies, f, indent=4)\n",
    "    del policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef21104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are this institute's rules or policies on maintaining academic integrity in the context of AI use, specifically regarding plagiarism and unauthorized assistance in academic work?\n",
      "1. **Policy**: Prohibition of AI-Generated Content as Original Work  \n",
      "   **Description**: Students are strictly prohibited from submitting AI-generated content as their own work without proper attribution. Misrepresenting AI-generated material as original work constitutes plagiarism and is treated as a violation of academic integrity. Students must disclose any use of AI tools in their assignments.  \n",
      "   **Examples**: University of California, Santa Barbara; Johns Hopkins University; University of Chicago; University of Florida; University of Michigan--Ann Arbor.\n",
      "\n",
      "2. **Policy**: Requirement for Proper Attribution of AI Contributions  \n",
      "   **Description**: Institutions require that any use of AI-generated content must be properly cited to avoid plagiarism. Students are expected to acknowledge AI contributions in their work, similar to traditional sources, to maintain academic integrity standards.  \n",
      "   **Examples**: University of California, Davis; University of Michigan--Ann Arbor; Yale University; University of Virginia; University of California, Los Angeles.\n",
      "\n",
      "3. **Policy**: Instructor Discretion on AI Use  \n",
      "   **Description**: Instructors have the authority to define and communicate their own policies regarding the use of AI tools in their courses. This includes specifying whether AI use is permitted, under what conditions, and the consequences for violations. Students are encouraged to clarify these policies with their instructors.  \n",
      "   **Examples**: Stanford University; University of Southern California; Vanderbilt University; Carnegie Mellon University; University of Pennsylvania.\n",
      "\n",
      "4. **Policy**: Clear Communication of AI Use Expectations  \n",
      "   **Description**: Universities emphasize the importance of clearly communicating expectations regarding AI use in course syllabi. Instructors are encouraged to outline what constitutes acceptable and unacceptable use of AI tools, helping to prevent misunderstandings related to academic integrity.  \n",
      "   **Examples**: University of Florida; Harvard University; University of Wisconsin Madison; Cornell University; University of Maryland.\n",
      "\n",
      "5. **Policy**: Disciplinary Actions for Violations  \n",
      "   **Description**: Violations of AI usage policies, including unauthorized use of AI tools and failure to properly attribute AI-generated content, may lead to disciplinary actions as outlined in the institution's academic integrity framework. This can include failing grades or referrals to academic integrity boards.  \n",
      "   **Examples**: Georgia Institute of Technology; University of Notre Dame; University of Rochester; Texas A&M University; University of Georgia.\n",
      "\n",
      "6. **Policy**: Emphasis on Ethical Use and Education  \n",
      "   **Description**: Institutions focus on educating students about the ethical use of AI tools, encouraging responsible practices and critical thinking. This includes discussions about the implications of AI use in academic work and the importance of maintaining academic integrity.  \n",
      "   **Examples**: University of Florida; University of Maryland; University of Illinois Urbana-Champaign; Texas A&M University; University of Wisconsin Madison.\n",
      "----------------------------------------\n",
      "What guidelines does this institute provide for instructors to effectively communicate expectations for AI use in coursework?\n",
      "1. **Policy**: Clear Communication of AI Use Expectations  \n",
      "   **Description**: Instructors are required to clearly outline their expectations regarding AI use in their syllabi, specifying what is permissible and what is not. This includes detailing acceptable AI tools, the conditions under which they can be used, and the importance of citing AI-generated content. Regular discussions about these expectations throughout the course are encouraged to reinforce understanding.  \n",
      "   **Examples**: University of Michigan--Ann Arbor, University of Florida, Yale University, University of California, Davis, Johns Hopkins University.\n",
      "\n",
      "2. **Policy**: Tailored AI Policies for Specific Courses  \n",
      "   **Description**: Instructors are encouraged to develop specific AI policies tailored to their individual courses, which can include guidelines on permissible AI use, ethical considerations, and the integration of AI tools into assignments. This approach allows for meaningful discussions about AI's role in learning and aligns with course objectives.  \n",
      "   **Examples**: University of Illinois Urbana-Champaign, Stanford University, Texas A&M University, University of California, Santa Barbara, University of Notre Dame.\n",
      "\n",
      "3. **Policy**: Integration of Ethical Discussions on AI  \n",
      "   **Description**: Instructors are encouraged to integrate discussions about the ethical implications of AI use and its impact on learning into their course framework. This includes fostering critical thinking about AI tools, their limitations, and the importance of responsible usage.  \n",
      "   **Examples**: Duke University, University of Southern California, Georgia Institute of Technology, University of California, Berkeley, University of Florida.\n",
      "\n",
      "4. **Policy**: Documentation and Citation of AI Use  \n",
      "   **Description**: Students are required to document and cite any AI-generated content they use in their assignments. Instructors should provide clear guidelines on how to properly acknowledge AI contributions, emphasizing the importance of academic integrity and transparency.  \n",
      "   **Examples**: Harvard University, Johns Hopkins University, University of Notre Dame, Texas A&M University, University of California, Davis.\n",
      "\n",
      "5. **Policy**: Ongoing Engagement with AI Policies  \n",
      "   **Description**: Instructors should maintain transparency about their AI policies and engage students in ongoing discussions about these guidelines throughout the semester. This includes revisiting policies regularly to ensure understanding and compliance.  \n",
      "   **Examples**: University of Wisconsin Madison, Vanderbilt University, Boston University, University of Virginia.\n",
      "----------------------------------------\n",
      "What policies or guidelines does this institute have for addressing data privacy and security concerns when inputting sensitive or confidential information into AI systems?\n",
      "1. **Policy**: Prohibition of Sensitive Data Input into AI Tools  \n",
      "   **Description**: Most universities prohibit the input of sensitive or confidential information, such as personally identifiable information (PII), health data, and student records, into AI systems. Users are advised to only use publicly available or low-risk data when interacting with AI tools.  \n",
      "   **Examples**: University of Michigan--Ann Arbor, Harvard University, University of Florida, University of California, Davis, Vanderbilt University.\n",
      "\n",
      "2. **Policy**: Requirement for Security and Privacy Reviews  \n",
      "   **Description**: Institutions require that any generative AI tools used for processing sensitive data undergo a security and privacy review before use. This includes ensuring compliance with existing privacy laws (e.g., FERPA, HIPAA) and institutional policies. Users must consult with relevant offices to verify that the tools meet security and privacy standards.  \n",
      "   **Examples**: University of California, Los Angeles, University of North Carolina at Chapel Hill, Stanford University, University of Virginia, University of California at Irvine.\n",
      "\n",
      "3. **Policy**: User Responsibility and Ethical Considerations  \n",
      "   **Description**: Users are held accountable for the data they input into AI systems. They must ensure that they understand the implications of using AI tools, including the potential for data breaches and ethical considerations surrounding data privacy. Users are encouraged to consult with relevant offices if unsure about data usage.  \n",
      "   **Examples**: Vanderbilt University, University of Southern California, Texas A&M University, University of Maryland, University of Pennsylvania.\n",
      "\n",
      "4. **Policy**: Informed Consent and Data Use Agreements  \n",
      "   **Description**: Some universities require that informed consent be obtained from individuals before their data is used in AI systems. Additionally, Data Use Agreements must be established to ensure proper privacy and confidentiality terms when using AI tools, particularly for research involving sensitive data.  \n",
      "   **Examples**: University of California, Los Angeles, University of Florida, University of Pennsylvania, University of Notre Dame.\n",
      "\n",
      "5. **Policy**: Emphasis on Anonymization and Data Minimization  \n",
      "   **Description**: Institutions advocate for the anonymization of data before it is input into AI systems and recommend using only the minimum necessary data to achieve the intended purpose. This approach aims to reduce the risk of exposing sensitive information and comply with data protection regulations.  \n",
      "   **Examples**: Yale University, University of California, San Diego, Massachusetts Institute of Technology, University of Rochester.\n",
      "----------------------------------------\n",
      "What ethical considerations does this institute have for addressing concerns such as bias, misinformation, and the perpetuation of inequalities in AI use?\n",
      "1. **Policy**: Transparency and Disclosure of AI Usage  \n",
      "   **Description**: Many universities require clear disclosure of AI usage in academic work and decision-making processes. This policy aims to promote accountability and ensure that AI-generated content is validated for accuracy, thereby addressing concerns related to bias and misinformation.  \n",
      "   **Examples**: University of Pennsylvania, University of California, San Diego, University of Michigan--Ann Arbor.\n",
      "\n",
      "2. **Policy**: Bias Audits and Evaluation of AI Tools  \n",
      "   **Description**: Institutions conduct regular bias audits and evaluations of AI tools to identify and address potential biases and inaccuracies. This proactive approach aims to ensure fairness in AI applications and protect marginalized groups from algorithmic discrimination.  \n",
      "   **Examples**: Texas A&M University, University of California, Davis, University of Florida.\n",
      "\n",
      "3. **Policy**: Ethical Guidelines and Frameworks for AI Use  \n",
      "   **Description**: Universities are developing and implementing ethical guidelines that govern the use of AI in educational settings. These guidelines address issues such as privacy, equity, and the potential for misinformation, ensuring that AI tools are used responsibly and ethically.  \n",
      "   **Examples**: Harvard University, Georgetown University, University of Virginia.\n",
      "\n",
      "4. **Policy**: Promotion of Critical Thinking and AI Literacy  \n",
      "   **Description**: Institutions emphasize the importance of fostering critical thinking and AI literacy among students. This includes educating students about the limitations and potential biases of AI tools, encouraging them to analyze AI-generated content critically, and promoting discussions about the ethical implications of AI use.  \n",
      "   **Examples**: Boston University, Northwestern University, Duke University.\n",
      "\n",
      "5. **Policy**: Inclusive Practices and Diverse Perspectives in AI Development  \n",
      "   **Description**: Some universities focus on incorporating diverse perspectives and inclusive practices in the development and application of AI technologies. This policy aims to address biases and ensure that AI tools do not perpetuate existing inequalities.  \n",
      "   **Examples**: Emory University, Carnegie Mellon University, Lehigh University.\n",
      "----------------------------------------\n",
      "Does this institute commit to continuous adaptation or policy evaluation to meet educational needs amid the fast-evolving nature of AI? If so, how does it aim to regularly review and update its guidelines on AI usage?\n",
      "1. **Policy**: Regular Review and Update of AI Guidelines  \n",
      "   **Description**: Many universities commit to a structured process for regularly reviewing and updating their AI usage guidelines. This involves engaging with faculty and students to gather feedback and assess the effectiveness of existing policies in light of new developments in AI technology.  \n",
      "   **Examples**: University of California, Los Angeles; Princeton University; University of Michigan--Ann Arbor; University of Florida.\n",
      "\n",
      "2. **Policy**: Collaborative Feedback Mechanisms  \n",
      "   **Description**: Institutions establish collaborative feedback mechanisms, such as task forces or working groups, that include faculty, students, and administrative staff. These groups evaluate AI policies and make recommendations for updates based on ongoing discussions and emerging challenges in AI usage.  \n",
      "   **Examples**: University of Washington; Rutgers University - New Brunswick; Brown University; University of California, Davis.\n",
      "\n",
      "3. **Policy**: Ethical and Responsible AI Use Guidelines  \n",
      "   **Description**: Institutions focus on developing guidelines that emphasize ethical and responsible use of AI. This includes ensuring that AI tools are used in ways that align with academic integrity and institutional values, and that faculty are equipped to address ethical considerations in their courses.  \n",
      "   **Examples**: California Institute of Technology; Yale University; Texas A&M University; Dartmouth College.\n",
      "\n",
      "4. **Policy**: Continuous Community Engagement  \n",
      "   **Description**: Some universities prioritize ongoing community engagement through initiatives such as workshops, seminars, and informal discussions about AI. This approach fosters a culture of transparency and collaboration, allowing for real-time feedback and adaptation of AI policies based on community experiences and insights.  \n",
      "   **Examples**: Massachusetts Institute of Technology; Vanderbilt University; University of Maryland; Tufts University.\n",
      "\n",
      "5. **Policy**: Integration of AI in Curriculum Development  \n",
      "   **Description**: Some universities emphasize the integration of AI considerations into curriculum development and teaching practices. This includes encouraging faculty to adapt their pedagogical approaches based on the evolving capabilities of AI tools and to communicate these changes clearly to students.  \n",
      "   **Examples**: University of Southern California; University of North Carolina at Chapel Hill; University of Wisconsin Madison; Harvard University.\n",
      "----------------------------------------\n",
      "What requirements does this institute have for documenting the use of AI tools to ensure transparency on how AI contributes to academic work?\n",
      "1. **Policy**: Clear Documentation and Citation of AI Tools  \n",
      "   **Description**: Students are required to document their use of AI tools in their academic work, including specific citations for any AI-generated content. This documentation must clarify which parts of the work were produced by AI and which were the student's original contributions, including prompts used and the nature of AI's contributions.  \n",
      "   **Examples**: University of Wisconsin Madison, University of California, Davis, University of Southern California, Harvard University.\n",
      "\n",
      "2. **Policy**: Instructor Guidelines for AI Tool Usage  \n",
      "   **Description**: Instructors must clearly outline their policies regarding the use of AI tools in their course syllabi. This includes specifying acceptable uses, expectations for documentation, and the consequences of non-compliance. Instructors should engage students in discussions about these policies at the beginning of the course.  \n",
      "   **Examples**: Dartmouth College, Yale University, The Ohio State University, Columbia University.\n",
      "\n",
      "3. **Policy**: Transparency in AI Contributions  \n",
      "   **Description**: Students must be transparent about how AI tools contributed to their work, detailing the specific tool used and the nature of its output. This includes distinguishing between AI-generated content and the student's own work, ensuring that the role of AI is clearly communicated in academic submissions.  \n",
      "   **Examples**: Brown University, University of Michigan--Ann Arbor, University of California, Santa Barbara.\n",
      "\n",
      "4. **Policy**: Ethical Use and Compliance with Academic Integrity  \n",
      "   **Description**: Students must ensure that their use of AI tools complies with academic integrity policies. This includes obtaining permission from instructors before using AI tools for assignments and ensuring that AI-generated content is not misrepresented as original work. Violations of these policies may result in disciplinary action.  \n",
      "   **Examples**: Princeton University, Johns Hopkins University, University of Pennsylvania.\n",
      "\n",
      "5. **Policy**: Comprehensive Documentation of AI Usage  \n",
      "   **Description**: Students are required to provide comprehensive documentation of their interactions with AI tools, including prompts used, the nature of the AI's contributions, and any revisions made to AI-generated content. This documentation should be included as an appendix or in a designated section of their submissions.  \n",
      "   **Examples**: University of Texas at Austin, University of Southern California, Tufts University.\n",
      "----------------------------------------\n",
      "What level of discretion does this institute grant instructors in establishing their own guidelines for AI usage in academic settings?\n",
      "1. **Policy**: Significant Discretion with Clear Communication  \n",
      "   **Description**: Instructors are granted significant discretion to establish their own guidelines for AI usage in academic settings. They are required to clearly communicate their specific policies regarding acceptable and unacceptable uses of AI to students, typically through syllabi and course materials. This approach allows for a variety of policies tailored to individual course objectives and teaching philosophies.  \n",
      "   **Examples**: Stanford University, University of California, Los Angeles, Carnegie Mellon University, University of Florida, University of Wisconsin Madison, University of Michigan--Ann Arbor, University of Virginia, University of Southern California, University of Chicago, Texas A&M University.\n",
      "\n",
      "2. **Policy**: Default Prohibition with Instructor Autonomy  \n",
      "   **Description**: Instructors have the authority to establish their own guidelines for AI usage, with a general default prohibition on the use of AI tools unless explicitly permitted. This policy emphasizes the need for instructors to communicate their specific guidelines clearly to avoid misunderstandings.  \n",
      "   **Examples**: Dartmouth College, University of Virginia, University of Chicago, Texas A&M University.\n",
      "\n",
      "3. **Policy**: Encouragement of Ethical AI Discussions  \n",
      "   **Description**: Instructors are encouraged to integrate discussions about the ethical use of AI tools into their courses while establishing their own guidelines. This approach promotes academic integrity and responsible use of technology among students.  \n",
      "   **Examples**: Texas A&M University, The Ohio State University, University of Southern California.\n",
      "\n",
      "4. **Policy**: Flexibility with Institutional Guidance  \n",
      "   **Description**: Instructors are allowed to establish their own guidelines for AI usage, but they are encouraged to refer to institutional resources or examples for guidance. This flexibility allows for a variety of approaches while still providing a framework for decision-making.  \n",
      "   **Examples**: University of Illinois Urbana-Champaign, University of California, Santa Barbara, Boston University, Johns Hopkins University.\n",
      "\n",
      "These policies reflect a range of approaches taken by universities, emphasizing the significant discretion granted to instructors while also highlighting the importance of clear communication and ethical considerations in developing AI usage guidelines.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in queries:\n",
    "    print(i)\n",
    "    print(synthesized_policies[i])\n",
    "    print(\"----\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
